{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import word2vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news-bm.json','r') as fopen:\n",
    "    sentences = json.loads(fopen.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array, dictionary, rev_dictionary, num_lines, num_words = word2vec.build_word_array(sentences,vocab_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = word2vec.build_training_set(word_array)\n",
    "graph_params = {'batch_size': 32,\n",
    "                'vocab_size': np.max(X)+1,\n",
    "                'embed_size': 32,\n",
    "                'hid_size': 32,\n",
    "                'neg_samples': 64,\n",
    "                'learn_rate': 0.01,\n",
    "                'momentum': 0.9,\n",
    "                'embed_noise': 0.1,\n",
    "                'hid_noise': 0.3,\n",
    "                'epoch':500,\n",
    "                'optimizer': 'Momentum'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = round(X.shape[0]*0.9)\n",
    "X_val, Y_val = (X[split:, :], Y[split:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model built, vocab size 33735, document length 1211339\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Model(graph_params)\n",
    "print('model built, vocab size %d, document length %d'%(np.max(X)+1, len(word_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, total batch 37854, train loss 37.301190, val loss 7.690268\n",
      "epoch 2, total batch 41639, train loss 7.380806, val loss 6.989402\n",
      "epoch 3, total batch 45424, train loss 6.830880, val loss 6.590046\n",
      "epoch 4, total batch 49209, train loss 6.499525, val loss 6.301902\n",
      "epoch 5, total batch 52994, train loss 6.294592, val loss 6.102567\n",
      "epoch 6, total batch 56779, train loss 6.103857, val loss 5.952074\n",
      "epoch 7, total batch 60564, train loss 6.013176, val loss 5.873894\n",
      "epoch 8, total batch 64349, train loss 5.963554, val loss 5.822234\n",
      "epoch 9, total batch 68134, train loss 5.906032, val loss 5.801923\n",
      "epoch 10, total batch 71919, train loss 5.888683, val loss 5.780524\n",
      "epoch 11, total batch 75704, train loss 5.848847, val loss 5.758367\n",
      "epoch 12, total batch 79489, train loss 5.826256, val loss 5.746864\n",
      "epoch 13, total batch 83274, train loss 5.821108, val loss 5.737143\n",
      "epoch 14, total batch 87059, train loss 5.809852, val loss 5.723542\n",
      "epoch 15, total batch 90844, train loss 5.790238, val loss 5.706316\n",
      "epoch 16, total batch 94629, train loss 5.791596, val loss 5.704609\n",
      "epoch 17, total batch 98414, train loss 5.746927, val loss 5.680347\n",
      "epoch 18, total batch 102199, train loss 5.757251, val loss 5.676268\n",
      "epoch 19, total batch 105984, train loss 5.754054, val loss 5.674141\n",
      "epoch 20, total batch 109769, train loss 5.736051, val loss 5.662402\n",
      "epoch 21, total batch 113554, train loss 5.715225, val loss 5.660530\n",
      "epoch 22, total batch 117339, train loss 5.723470, val loss 5.648937\n",
      "epoch 23, total batch 121124, train loss 5.702702, val loss 5.629691\n",
      "epoch 24, total batch 124909, train loss 5.698489, val loss 5.617675\n",
      "epoch 25, total batch 128694, train loss 5.683477, val loss 5.630019\n",
      "epoch 26, total batch 132479, train loss 5.674368, val loss 5.608154\n",
      "epoch 27, total batch 136264, train loss 5.666263, val loss 5.604952\n",
      "epoch 28, total batch 140049, train loss 5.671454, val loss 5.593897\n",
      "epoch 29, total batch 143834, train loss 5.658314, val loss 5.595180\n",
      "epoch 30, total batch 147619, train loss 5.649436, val loss 5.581293\n",
      "epoch 31, total batch 151404, train loss 5.640098, val loss 5.571337\n",
      "epoch 32, total batch 155189, train loss 5.625918, val loss 5.569661\n",
      "epoch 33, total batch 158974, train loss 5.623571, val loss 5.558435\n",
      "epoch 34, total batch 162759, train loss 5.625353, val loss 5.548818\n",
      "epoch 35, total batch 166544, train loss 5.614698, val loss 5.553186\n",
      "epoch 36, total batch 170329, train loss 5.611311, val loss 5.550554\n",
      "epoch 37, total batch 174114, train loss 5.594470, val loss 5.538892\n",
      "epoch 38, total batch 177899, train loss 5.587693, val loss 5.539577\n",
      "epoch 39, total batch 181684, train loss 5.585279, val loss 5.530500\n",
      "epoch 40, total batch 185469, train loss 5.580919, val loss 5.526388\n",
      "epoch 41, total batch 189254, train loss 5.576245, val loss 5.537573\n",
      "epoch 42, total batch 193039, train loss 5.579389, val loss 5.511732\n",
      "epoch 43, total batch 196824, train loss 5.568208, val loss 5.501549\n",
      "epoch 44, total batch 200609, train loss 5.554364, val loss 5.505231\n",
      "epoch 45, total batch 204394, train loss 5.544937, val loss 5.502451\n",
      "epoch 46, total batch 208179, train loss 5.542935, val loss 5.500065\n",
      "epoch 47, total batch 211964, train loss 5.537580, val loss 5.484989\n",
      "epoch 48, total batch 215749, train loss 5.545733, val loss 5.489142\n",
      "epoch 49, total batch 219534, train loss 5.532777, val loss 5.492346\n",
      "epoch 50, total batch 223319, train loss 5.538167, val loss 5.473334\n",
      "epoch 51, total batch 227104, train loss 5.535242, val loss 5.470325\n",
      "epoch 52, total batch 230889, train loss 5.524405, val loss 5.462316\n",
      "epoch 53, total batch 234674, train loss 5.507344, val loss 5.465152\n",
      "epoch 54, total batch 238459, train loss 5.524363, val loss 5.457274\n",
      "epoch 55, total batch 242244, train loss 5.525803, val loss 5.463158\n",
      "epoch 56, total batch 246029, train loss 5.500397, val loss 5.464006\n",
      "epoch 57, total batch 249814, train loss 5.495718, val loss 5.466745\n",
      "epoch 58, total batch 253599, train loss 5.509182, val loss 5.446456\n",
      "epoch 59, total batch 257384, train loss 5.493936, val loss 5.453538\n",
      "epoch 60, total batch 261169, train loss 5.498673, val loss 5.443223\n",
      "epoch 61, total batch 264954, train loss 5.487897, val loss 5.445435\n",
      "epoch 62, total batch 268739, train loss 5.472274, val loss 5.440322\n",
      "epoch 63, total batch 272524, train loss 5.487418, val loss 5.426173\n",
      "epoch 64, total batch 276309, train loss 5.479708, val loss 5.426365\n",
      "epoch 65, total batch 280094, train loss 5.465982, val loss 5.429245\n",
      "epoch 66, total batch 283879, train loss 5.478995, val loss 5.426075\n",
      "epoch 67, total batch 287664, train loss 5.471622, val loss 5.423189\n",
      "epoch 68, total batch 291449, train loss 5.473046, val loss 5.426095\n",
      "epoch 69, total batch 295234, train loss 5.468317, val loss 5.413647\n",
      "epoch 70, total batch 299019, train loss 5.457540, val loss 5.407573\n",
      "epoch 71, total batch 302804, train loss 5.465922, val loss 5.413254\n",
      "epoch 72, total batch 306589, train loss 5.444444, val loss 5.407693\n",
      "epoch 73, total batch 310374, train loss 5.444338, val loss 5.411078\n",
      "epoch 74, total batch 314159, train loss 5.442605, val loss 5.403242\n",
      "epoch 75, total batch 317944, train loss 5.438896, val loss 5.394149\n",
      "epoch 76, total batch 321729, train loss 5.429809, val loss 5.397433\n",
      "epoch 77, total batch 325514, train loss 5.409370, val loss 5.361489\n",
      "epoch 78, total batch 329299, train loss 5.382080, val loss 5.344173\n",
      "epoch 79, total batch 333084, train loss 5.362962, val loss 5.306936\n",
      "epoch 80, total batch 336869, train loss 5.312883, val loss 5.243311\n",
      "epoch 81, total batch 340654, train loss 5.237520, val loss 5.164597\n",
      "epoch 82, total batch 344439, train loss 5.162720, val loss 5.104236\n",
      "epoch 83, total batch 348224, train loss 5.089731, val loss 5.048692\n",
      "epoch 84, total batch 352009, train loss 5.032723, val loss 4.983548\n",
      "epoch 85, total batch 355794, train loss 4.961300, val loss 4.921007\n",
      "epoch 86, total batch 359579, train loss 4.884469, val loss 4.839477\n",
      "epoch 87, total batch 363364, train loss 4.815252, val loss 4.783673\n",
      "epoch 88, total batch 367149, train loss 4.754939, val loss 4.733633\n",
      "epoch 89, total batch 370934, train loss 4.707066, val loss 4.671852\n",
      "epoch 90, total batch 374719, train loss 4.659120, val loss 4.638457\n",
      "epoch 91, total batch 378504, train loss 4.610822, val loss 4.597511\n",
      "epoch 92, total batch 382289, train loss 4.577073, val loss 4.558892\n",
      "epoch 93, total batch 386074, train loss 4.546677, val loss 4.530012\n",
      "epoch 94, total batch 389859, train loss 4.517231, val loss 4.504135\n",
      "epoch 95, total batch 393644, train loss 4.480592, val loss 4.470283\n",
      "epoch 96, total batch 397429, train loss 4.461318, val loss 4.446902\n",
      "epoch 97, total batch 401214, train loss 4.421580, val loss 4.421791\n",
      "epoch 98, total batch 404999, train loss 4.397600, val loss 4.389247\n",
      "epoch 99, total batch 408784, train loss 4.371440, val loss 4.359003\n",
      "epoch 100, total batch 412569, train loss 4.333465, val loss 4.342915\n",
      "epoch 101, total batch 416354, train loss 4.308805, val loss 4.308270\n",
      "epoch 102, total batch 420139, train loss 4.268000, val loss 4.280642\n",
      "epoch 103, total batch 423924, train loss 4.245894, val loss 4.255254\n",
      "epoch 104, total batch 427709, train loss 4.215996, val loss 4.226789\n",
      "epoch 105, total batch 431494, train loss 4.183816, val loss 4.203242\n",
      "epoch 106, total batch 435279, train loss 4.156177, val loss 4.174792\n",
      "epoch 107, total batch 439064, train loss 4.141659, val loss 4.156140\n",
      "epoch 108, total batch 442849, train loss 4.113696, val loss 4.129796\n",
      "epoch 109, total batch 446634, train loss 4.089612, val loss 4.109332\n",
      "epoch 110, total batch 450419, train loss 4.062666, val loss 4.086020\n",
      "epoch 111, total batch 454204, train loss 4.047838, val loss 4.070843\n",
      "epoch 112, total batch 457989, train loss 4.025586, val loss 4.049681\n",
      "epoch 113, total batch 461774, train loss 4.008491, val loss 4.029713\n",
      "epoch 114, total batch 465559, train loss 3.991455, val loss 4.007457\n",
      "epoch 115, total batch 469344, train loss 3.982211, val loss 3.997869\n",
      "epoch 116, total batch 473129, train loss 3.954771, val loss 3.975311\n",
      "epoch 117, total batch 476914, train loss 3.941609, val loss 3.966311\n",
      "epoch 118, total batch 480699, train loss 3.927277, val loss 3.952134\n",
      "epoch 119, total batch 484484, train loss 3.904391, val loss 3.934075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120, total batch 488269, train loss 3.899969, val loss 3.919995\n",
      "epoch 121, total batch 492054, train loss 3.888624, val loss 3.914137\n",
      "epoch 122, total batch 495839, train loss 3.875825, val loss 3.897872\n",
      "epoch 123, total batch 499624, train loss 3.854002, val loss 3.892038\n",
      "epoch 124, total batch 503409, train loss 3.845131, val loss 3.863005\n",
      "epoch 125, total batch 507194, train loss 3.830444, val loss 3.857252\n",
      "epoch 126, total batch 510979, train loss 3.828110, val loss 3.848102\n",
      "epoch 127, total batch 514764, train loss 3.805835, val loss 3.828006\n",
      "epoch 128, total batch 518549, train loss 3.795262, val loss 3.822739\n",
      "epoch 129, total batch 522334, train loss 3.777584, val loss 3.809908\n",
      "epoch 130, total batch 526119, train loss 3.771113, val loss 3.799022\n",
      "epoch 131, total batch 529904, train loss 3.762764, val loss 3.794665\n",
      "epoch 132, total batch 533689, train loss 3.751357, val loss 3.785353\n",
      "epoch 133, total batch 537474, train loss 3.743454, val loss 3.778274\n",
      "epoch 134, total batch 541259, train loss 3.729416, val loss 3.770669\n",
      "epoch 135, total batch 545044, train loss 3.725338, val loss 3.756782\n",
      "epoch 136, total batch 548829, train loss 3.720633, val loss 3.746480\n",
      "epoch 137, total batch 552614, train loss 3.710578, val loss 3.746943\n",
      "epoch 138, total batch 556399, train loss 3.705381, val loss 3.730995\n",
      "epoch 139, total batch 560184, train loss 3.688025, val loss 3.720117\n",
      "epoch 140, total batch 563969, train loss 3.682708, val loss 3.724919\n",
      "epoch 141, total batch 567754, train loss 3.682037, val loss 3.710774\n",
      "epoch 142, total batch 571539, train loss 3.665374, val loss 3.702019\n",
      "epoch 143, total batch 575324, train loss 3.667606, val loss 3.697088\n",
      "epoch 144, total batch 579109, train loss 3.649015, val loss 3.700124\n",
      "epoch 145, total batch 582894, train loss 3.651565, val loss 3.683309\n",
      "epoch 146, total batch 586679, train loss 3.642916, val loss 3.676820\n",
      "epoch 147, total batch 590464, train loss 3.638863, val loss 3.672503\n",
      "epoch 148, total batch 594249, train loss 3.626207, val loss 3.661259\n",
      "epoch 149, total batch 598034, train loss 3.623130, val loss 3.658778\n",
      "epoch 150, total batch 601819, train loss 3.613745, val loss 3.656644\n",
      "epoch 151, total batch 605604, train loss 3.609112, val loss 3.656358\n",
      "epoch 152, total batch 609389, train loss 3.593045, val loss 3.649375\n",
      "epoch 153, total batch 613174, train loss 3.586200, val loss 3.634259\n",
      "epoch 154, total batch 616959, train loss 3.591415, val loss 3.629856\n",
      "epoch 155, total batch 620744, train loss 3.578775, val loss 3.623098\n",
      "epoch 156, total batch 624529, train loss 3.578129, val loss 3.612565\n",
      "epoch 157, total batch 628314, train loss 3.563050, val loss 3.609760\n",
      "epoch 158, total batch 632099, train loss 3.560861, val loss 3.597427\n",
      "epoch 159, total batch 635884, train loss 3.560275, val loss 3.594805\n",
      "epoch 160, total batch 639669, train loss 3.545942, val loss 3.584592\n",
      "epoch 161, total batch 643454, train loss 3.537979, val loss 3.587184\n",
      "epoch 162, total batch 647239, train loss 3.530461, val loss 3.573571\n",
      "epoch 163, total batch 651024, train loss 3.520107, val loss 3.560661\n",
      "epoch 164, total batch 654809, train loss 3.521396, val loss 3.562331\n",
      "epoch 165, total batch 658594, train loss 3.520986, val loss 3.555135\n",
      "epoch 166, total batch 662379, train loss 3.508459, val loss 3.552170\n",
      "epoch 167, total batch 666164, train loss 3.503323, val loss 3.545633\n",
      "epoch 168, total batch 669949, train loss 3.491092, val loss 3.537513\n",
      "epoch 169, total batch 673734, train loss 3.492606, val loss 3.533799\n",
      "epoch 170, total batch 677519, train loss 3.482452, val loss 3.529581\n",
      "epoch 171, total batch 681304, train loss 3.486735, val loss 3.518453\n",
      "epoch 172, total batch 685089, train loss 3.471910, val loss 3.511888\n",
      "epoch 173, total batch 688874, train loss 3.463764, val loss 3.508841\n",
      "epoch 174, total batch 692659, train loss 3.457419, val loss 3.508786\n",
      "epoch 175, total batch 696444, train loss 3.448635, val loss 3.503070\n",
      "epoch 176, total batch 700229, train loss 3.453484, val loss 3.496186\n",
      "epoch 177, total batch 704014, train loss 3.443239, val loss 3.488318\n",
      "epoch 178, total batch 707799, train loss 3.433213, val loss 3.484427\n",
      "epoch 179, total batch 711584, train loss 3.433060, val loss 3.475951\n",
      "epoch 180, total batch 715369, train loss 3.437378, val loss 3.469651\n",
      "epoch 181, total batch 719154, train loss 3.426227, val loss 3.476547\n",
      "epoch 182, total batch 722939, train loss 3.409635, val loss 3.460710\n",
      "epoch 183, total batch 726724, train loss 3.414326, val loss 3.455606\n",
      "epoch 184, total batch 730509, train loss 3.411562, val loss 3.455797\n",
      "epoch 185, total batch 734294, train loss 3.396730, val loss 3.444706\n",
      "epoch 186, total batch 738079, train loss 3.395233, val loss 3.444265\n",
      "epoch 187, total batch 741864, train loss 3.392273, val loss 3.437577\n",
      "epoch 188, total batch 745649, train loss 3.382535, val loss 3.436228\n",
      "epoch 189, total batch 749434, train loss 3.390641, val loss 3.427556\n",
      "epoch 190, total batch 753219, train loss 3.379107, val loss 3.424891\n",
      "epoch 191, total batch 757004, train loss 3.370153, val loss 3.414899\n",
      "epoch 192, total batch 760789, train loss 3.379570, val loss 3.413650\n",
      "epoch 193, total batch 764574, train loss 3.364969, val loss 3.415011\n",
      "epoch 194, total batch 768359, train loss 3.365367, val loss 3.395864\n",
      "epoch 195, total batch 772144, train loss 3.357697, val loss 3.393639\n",
      "epoch 196, total batch 775929, train loss 3.358504, val loss 3.397463\n",
      "epoch 197, total batch 779714, train loss 3.348003, val loss 3.392895\n",
      "epoch 198, total batch 783499, train loss 3.339648, val loss 3.386646\n",
      "epoch 199, total batch 787284, train loss 3.345328, val loss 3.392106\n",
      "epoch 200, total batch 791069, train loss 3.339731, val loss 3.382851\n",
      "epoch 201, total batch 794854, train loss 3.331886, val loss 3.381709\n",
      "epoch 202, total batch 798639, train loss 3.323575, val loss 3.372008\n",
      "epoch 203, total batch 802424, train loss 3.322512, val loss 3.372351\n",
      "epoch 204, total batch 806209, train loss 3.321893, val loss 3.366780\n",
      "epoch 205, total batch 809994, train loss 3.322378, val loss 3.366893\n",
      "epoch 206, total batch 813779, train loss 3.315640, val loss 3.364513\n",
      "epoch 207, total batch 817564, train loss 3.316924, val loss 3.353735\n",
      "epoch 208, total batch 821349, train loss 3.314572, val loss 3.349262\n",
      "epoch 209, total batch 825134, train loss 3.306797, val loss 3.348465\n",
      "epoch 210, total batch 828919, train loss 3.292837, val loss 3.345418\n",
      "epoch 211, total batch 832704, train loss 3.297113, val loss 3.345695\n",
      "epoch 212, total batch 836489, train loss 3.298465, val loss 3.339923\n",
      "epoch 213, total batch 840274, train loss 3.289837, val loss 3.327623\n",
      "epoch 214, total batch 844059, train loss 3.288392, val loss 3.325470\n",
      "epoch 215, total batch 847844, train loss 3.289915, val loss 3.323440\n",
      "epoch 216, total batch 851629, train loss 3.272522, val loss 3.319076\n",
      "epoch 217, total batch 855414, train loss 3.291648, val loss 3.322529\n",
      "epoch 218, total batch 859199, train loss 3.269681, val loss 3.313428\n",
      "epoch 219, total batch 862984, train loss 3.275006, val loss 3.306594\n",
      "epoch 220, total batch 866769, train loss 3.269140, val loss 3.304777\n",
      "epoch 221, total batch 870554, train loss 3.262296, val loss 3.307794\n",
      "epoch 222, total batch 874339, train loss 3.251328, val loss 3.312160\n",
      "epoch 223, total batch 878124, train loss 3.268443, val loss 3.303103\n",
      "epoch 224, total batch 881909, train loss 3.257599, val loss 3.302737\n",
      "epoch 225, total batch 885694, train loss 3.254393, val loss 3.295908\n",
      "epoch 226, total batch 889479, train loss 3.247033, val loss 3.292337\n",
      "epoch 227, total batch 893264, train loss 3.241064, val loss 3.287509\n",
      "epoch 228, total batch 897049, train loss 3.246167, val loss 3.285548\n",
      "epoch 229, total batch 900834, train loss 3.242972, val loss 3.287106\n",
      "epoch 230, total batch 904619, train loss 3.229572, val loss 3.283275\n",
      "epoch 231, total batch 908404, train loss 3.230664, val loss 3.269753\n",
      "epoch 232, total batch 912189, train loss 3.237791, val loss 3.274934\n",
      "epoch 233, total batch 915974, train loss 3.226667, val loss 3.275723\n",
      "epoch 234, total batch 919759, train loss 3.228102, val loss 3.264533\n",
      "epoch 235, total batch 923544, train loss 3.235512, val loss 3.271616\n",
      "epoch 236, total batch 927329, train loss 3.219993, val loss 3.270521\n",
      "epoch 237, total batch 931114, train loss 3.222271, val loss 3.261576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 238, total batch 934899, train loss 3.227642, val loss 3.271389\n",
      "epoch 239, total batch 938684, train loss 3.212421, val loss 3.267088\n",
      "epoch 240, total batch 942469, train loss 3.215231, val loss 3.259414\n",
      "epoch 241, total batch 946254, train loss 3.212023, val loss 3.263825\n",
      "epoch 242, total batch 950039, train loss 3.214939, val loss 3.249461\n",
      "epoch 243, total batch 953824, train loss 3.209475, val loss 3.249348\n",
      "epoch 244, total batch 957609, train loss 3.213088, val loss 3.244944\n",
      "epoch 245, total batch 961394, train loss 3.201004, val loss 3.243962\n",
      "epoch 246, total batch 965179, train loss 3.208472, val loss 3.249755\n",
      "epoch 247, total batch 968964, train loss 3.194614, val loss 3.235695\n",
      "epoch 248, total batch 972749, train loss 3.188182, val loss 3.238386\n",
      "epoch 249, total batch 976534, train loss 3.187494, val loss 3.236407\n",
      "epoch 250, total batch 980319, train loss 3.196021, val loss 3.239832\n",
      "epoch 251, total batch 984104, train loss 3.192725, val loss 3.235268\n",
      "epoch 252, total batch 987889, train loss 3.185846, val loss 3.235840\n",
      "epoch 253, total batch 991674, train loss 3.172717, val loss 3.229271\n",
      "epoch 254, total batch 995459, train loss 3.177698, val loss 3.229973\n",
      "epoch 255, total batch 999244, train loss 3.175350, val loss 3.224123\n",
      "epoch 256, total batch 1003029, train loss 3.172307, val loss 3.224111\n",
      "epoch 257, total batch 1006814, train loss 3.171202, val loss 3.230091\n",
      "epoch 258, total batch 1010599, train loss 3.179791, val loss 3.224147\n",
      "epoch 259, total batch 1014384, train loss 3.167654, val loss 3.221386\n",
      "epoch 260, total batch 1018169, train loss 3.169654, val loss 3.212827\n",
      "epoch 261, total batch 1021954, train loss 3.178432, val loss 3.206137\n",
      "epoch 262, total batch 1025739, train loss 3.161383, val loss 3.205187\n",
      "epoch 263, total batch 1029524, train loss 3.157324, val loss 3.209420\n",
      "epoch 264, total batch 1033309, train loss 3.164633, val loss 3.206856\n",
      "epoch 265, total batch 1037094, train loss 3.161219, val loss 3.201644\n",
      "epoch 266, total batch 1040879, train loss 3.159702, val loss 3.203566\n",
      "epoch 267, total batch 1044664, train loss 3.171986, val loss 3.203123\n",
      "epoch 268, total batch 1048449, train loss 3.153830, val loss 3.203577\n",
      "epoch 269, total batch 1052234, train loss 3.156582, val loss 3.195518\n",
      "epoch 270, total batch 1056019, train loss 3.147846, val loss 3.196884\n",
      "epoch 271, total batch 1059804, train loss 3.153833, val loss 3.194316\n",
      "epoch 272, total batch 1063589, train loss 3.147936, val loss 3.189709\n",
      "epoch 273, total batch 1067374, train loss 3.146832, val loss 3.188201\n",
      "epoch 274, total batch 1071159, train loss 3.140857, val loss 3.187849\n",
      "epoch 275, total batch 1074944, train loss 3.148365, val loss 3.179752\n",
      "epoch 276, total batch 1078729, train loss 3.142376, val loss 3.186400\n",
      "epoch 277, total batch 1082514, train loss 3.146672, val loss 3.177863\n",
      "epoch 278, total batch 1086299, train loss 3.140184, val loss 3.182515\n",
      "epoch 279, total batch 1090084, train loss 3.140888, val loss 3.182115\n",
      "epoch 280, total batch 1093869, train loss 3.143066, val loss 3.171525\n",
      "epoch 281, total batch 1097654, train loss 3.138185, val loss 3.177540\n",
      "epoch 282, total batch 1101439, train loss 3.136076, val loss 3.177821\n",
      "epoch 283, total batch 1105224, train loss 3.129488, val loss 3.183740\n",
      "epoch 284, total batch 1109009, train loss 3.135265, val loss 3.179804\n",
      "epoch 285, total batch 1112794, train loss 3.121982, val loss 3.172348\n",
      "epoch 286, total batch 1116579, train loss 3.123218, val loss 3.176607\n",
      "epoch 287, total batch 1120364, train loss 3.113222, val loss 3.169491\n",
      "epoch 288, total batch 1124149, train loss 3.116212, val loss 3.168368\n",
      "epoch 289, total batch 1127934, train loss 3.115745, val loss 3.169350\n",
      "epoch 290, total batch 1131719, train loss 3.110533, val loss 3.160337\n",
      "epoch 291, total batch 1135504, train loss 3.111710, val loss 3.171303\n",
      "epoch 292, total batch 1139289, train loss 3.114015, val loss 3.161475\n",
      "epoch 293, total batch 1143074, train loss 3.119337, val loss 3.154702\n",
      "epoch 294, total batch 1146859, train loss 3.114477, val loss 3.158572\n",
      "epoch 295, total batch 1150644, train loss 3.107637, val loss 3.155697\n",
      "epoch 296, total batch 1154429, train loss 3.105967, val loss 3.157279\n",
      "epoch 297, total batch 1158214, train loss 3.105280, val loss 3.153656\n",
      "epoch 298, total batch 1161999, train loss 3.105560, val loss 3.152983\n",
      "epoch 299, total batch 1165784, train loss 3.103313, val loss 3.163679\n",
      "epoch 300, total batch 1169569, train loss 3.104074, val loss 3.145961\n",
      "epoch 301, total batch 1173354, train loss 3.107940, val loss 3.147424\n",
      "epoch 302, total batch 1177139, train loss 3.106989, val loss 3.146574\n",
      "epoch 303, total batch 1180924, train loss 3.090502, val loss 3.139742\n",
      "epoch 304, total batch 1184709, train loss 3.093320, val loss 3.150302\n",
      "epoch 305, total batch 1188494, train loss 3.093100, val loss 3.139523\n",
      "epoch 306, total batch 1192279, train loss 3.093255, val loss 3.142663\n",
      "epoch 307, total batch 1196064, train loss 3.093194, val loss 3.143435\n",
      "epoch 308, total batch 1199849, train loss 3.090405, val loss 3.137965\n",
      "epoch 309, total batch 1203634, train loss 3.084664, val loss 3.140876\n",
      "epoch 310, total batch 1207419, train loss 3.095654, val loss 3.134528\n",
      "epoch 311, total batch 1211204, train loss 3.087319, val loss 3.131799\n",
      "epoch 312, total batch 1214989, train loss 3.085169, val loss 3.129804\n",
      "epoch 313, total batch 1218774, train loss 3.085202, val loss 3.137353\n",
      "epoch 314, total batch 1222559, train loss 3.081303, val loss 3.128103\n",
      "epoch 315, total batch 1226344, train loss 3.074784, val loss 3.130184\n",
      "epoch 316, total batch 1230129, train loss 3.081027, val loss 3.132531\n",
      "epoch 317, total batch 1233914, train loss 3.082974, val loss 3.123851\n",
      "epoch 318, total batch 1237699, train loss 3.065601, val loss 3.127432\n",
      "epoch 319, total batch 1241484, train loss 3.074603, val loss 3.128507\n",
      "epoch 320, total batch 1245269, train loss 3.075537, val loss 3.132360\n",
      "epoch 321, total batch 1249054, train loss 3.067303, val loss 3.124575\n",
      "epoch 322, total batch 1252839, train loss 3.073193, val loss 3.120964\n",
      "epoch 323, total batch 1256624, train loss 3.067381, val loss 3.125480\n",
      "epoch 324, total batch 1260409, train loss 3.078695, val loss 3.123414\n",
      "epoch 325, total batch 1264194, train loss 3.067794, val loss 3.119523\n",
      "epoch 326, total batch 1267979, train loss 3.063457, val loss 3.119083\n",
      "epoch 327, total batch 1271764, train loss 3.069777, val loss 3.118419\n",
      "epoch 328, total batch 1275549, train loss 3.058295, val loss 3.115802\n",
      "epoch 329, total batch 1279334, train loss 3.060518, val loss 3.114626\n",
      "epoch 330, total batch 1283119, train loss 3.054834, val loss 3.116443\n",
      "epoch 331, total batch 1286904, train loss 3.055311, val loss 3.111473\n",
      "epoch 332, total batch 1290689, train loss 3.059289, val loss 3.110852\n",
      "epoch 333, total batch 1294474, train loss 3.063732, val loss 3.110865\n",
      "epoch 334, total batch 1298259, train loss 3.057361, val loss 3.112530\n",
      "epoch 335, total batch 1302044, train loss 3.059008, val loss 3.109274\n",
      "epoch 336, total batch 1305829, train loss 3.050196, val loss 3.111500\n",
      "epoch 337, total batch 1309614, train loss 3.049044, val loss 3.111195\n",
      "epoch 338, total batch 1313399, train loss 3.059172, val loss 3.110349\n",
      "epoch 339, total batch 1317184, train loss 3.060831, val loss 3.105753\n",
      "epoch 340, total batch 1320969, train loss 3.048737, val loss 3.109722\n",
      "epoch 341, total batch 1324754, train loss 3.053611, val loss 3.104383\n",
      "epoch 342, total batch 1328539, train loss 3.044685, val loss 3.097393\n",
      "epoch 343, total batch 1332324, train loss 3.047794, val loss 3.097460\n",
      "epoch 344, total batch 1336109, train loss 3.043410, val loss 3.098351\n",
      "epoch 345, total batch 1339894, train loss 3.047740, val loss 3.109694\n",
      "epoch 346, total batch 1343679, train loss 3.053160, val loss 3.094950\n",
      "epoch 347, total batch 1347464, train loss 3.039665, val loss 3.092432\n",
      "epoch 348, total batch 1351249, train loss 3.049709, val loss 3.097342\n",
      "epoch 349, total batch 1355034, train loss 3.038488, val loss 3.091055\n",
      "epoch 350, total batch 1358819, train loss 3.035661, val loss 3.098633\n",
      "epoch 351, total batch 1362604, train loss 3.043529, val loss 3.092856\n",
      "epoch 352, total batch 1366389, train loss 3.023719, val loss 3.092620\n",
      "epoch 353, total batch 1370174, train loss 3.038601, val loss 3.091028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 354, total batch 1373959, train loss 3.037313, val loss 3.091166\n",
      "epoch 355, total batch 1377744, train loss 3.047830, val loss 3.086793\n",
      "epoch 356, total batch 1381529, train loss 3.040950, val loss 3.087778\n",
      "epoch 357, total batch 1385314, train loss 3.040659, val loss 3.080090\n",
      "epoch 358, total batch 1389099, train loss 3.034903, val loss 3.085161\n",
      "epoch 359, total batch 1392884, train loss 3.030361, val loss 3.083946\n",
      "epoch 360, total batch 1396669, train loss 3.023674, val loss 3.083199\n",
      "epoch 361, total batch 1400454, train loss 3.025247, val loss 3.086472\n",
      "epoch 362, total batch 1404239, train loss 3.023869, val loss 3.079621\n",
      "epoch 363, total batch 1408024, train loss 3.031439, val loss 3.072012\n",
      "epoch 364, total batch 1411809, train loss 3.024874, val loss 3.078490\n",
      "epoch 365, total batch 1415594, train loss 3.024706, val loss 3.082422\n",
      "epoch 366, total batch 1419379, train loss 3.021383, val loss 3.074203\n",
      "epoch 367, total batch 1423164, train loss 3.027438, val loss 3.073870\n",
      "epoch 368, total batch 1426949, train loss 3.013985, val loss 3.074498\n",
      "epoch 369, total batch 1430734, train loss 3.030144, val loss 3.072636\n",
      "epoch 370, total batch 1434519, train loss 3.027372, val loss 3.074300\n",
      "epoch 371, total batch 1438304, train loss 3.021315, val loss 3.067820\n",
      "epoch 372, total batch 1442089, train loss 3.014451, val loss 3.065487\n",
      "epoch 373, total batch 1445874, train loss 3.017836, val loss 3.068742\n",
      "epoch 374, total batch 1449659, train loss 3.019557, val loss 3.061209\n",
      "epoch 375, total batch 1453444, train loss 3.022012, val loss 3.071254\n",
      "epoch 376, total batch 1457229, train loss 3.017291, val loss 3.063208\n",
      "epoch 377, total batch 1461014, train loss 3.012475, val loss 3.058687\n",
      "epoch 378, total batch 1464799, train loss 3.009867, val loss 3.061930\n",
      "epoch 379, total batch 1468584, train loss 3.013634, val loss 3.069042\n",
      "epoch 380, total batch 1472369, train loss 3.004493, val loss 3.066106\n",
      "epoch 381, total batch 1476154, train loss 3.003220, val loss 3.061433\n",
      "epoch 382, total batch 1479939, train loss 3.004669, val loss 3.055184\n",
      "epoch 383, total batch 1483724, train loss 3.004493, val loss 3.070454\n",
      "epoch 384, total batch 1487509, train loss 3.002487, val loss 3.061958\n",
      "epoch 385, total batch 1491294, train loss 3.001680, val loss 3.058322\n",
      "epoch 386, total batch 1495079, train loss 2.998176, val loss 3.061892\n",
      "epoch 387, total batch 1498864, train loss 3.004353, val loss 3.057023\n",
      "epoch 388, total batch 1502649, train loss 3.005319, val loss 3.058254\n",
      "epoch 389, total batch 1506434, train loss 2.999833, val loss 3.062337\n",
      "epoch 390, total batch 1510219, train loss 3.001565, val loss 3.054081\n",
      "epoch 391, total batch 1514004, train loss 2.996061, val loss 3.048735\n",
      "epoch 392, total batch 1517789, train loss 2.992723, val loss 3.049923\n",
      "epoch 393, total batch 1521574, train loss 2.996302, val loss 3.047477\n",
      "epoch 394, total batch 1525359, train loss 2.986765, val loss 3.047760\n",
      "epoch 395, total batch 1529144, train loss 2.994055, val loss 3.045031\n",
      "epoch 396, total batch 1532929, train loss 2.998946, val loss 3.052961\n",
      "epoch 397, total batch 1536714, train loss 3.001933, val loss 3.040949\n",
      "epoch 398, total batch 1540499, train loss 2.993833, val loss 3.051156\n",
      "epoch 399, total batch 1544284, train loss 2.989511, val loss 3.046360\n",
      "epoch 400, total batch 1548069, train loss 2.986141, val loss 3.039662\n",
      "epoch 401, total batch 1551854, train loss 2.985410, val loss 3.040464\n",
      "epoch 402, total batch 1555639, train loss 2.982232, val loss 3.041023\n",
      "epoch 403, total batch 1559424, train loss 2.979079, val loss 3.038923\n",
      "epoch 404, total batch 1563209, train loss 2.981067, val loss 3.042686\n",
      "epoch 405, total batch 1566994, train loss 2.977874, val loss 3.037646\n",
      "epoch 406, total batch 1570779, train loss 2.980650, val loss 3.044532\n",
      "epoch 407, total batch 1574564, train loss 2.977207, val loss 3.047569\n",
      "epoch 408, total batch 1578349, train loss 2.985093, val loss 3.040201\n",
      "epoch 409, total batch 1582134, train loss 2.981528, val loss 3.029936\n",
      "epoch 410, total batch 1585919, train loss 2.975668, val loss 3.037998\n",
      "epoch 411, total batch 1589704, train loss 2.976601, val loss 3.040247\n",
      "epoch 412, total batch 1593489, train loss 2.981045, val loss 3.036845\n",
      "epoch 413, total batch 1597274, train loss 2.977817, val loss 3.033244\n",
      "epoch 414, total batch 1601059, train loss 2.970539, val loss 3.036530\n",
      "epoch 415, total batch 1604844, train loss 2.968620, val loss 3.043819\n",
      "epoch 416, total batch 1608629, train loss 2.978966, val loss 3.028304\n",
      "epoch 417, total batch 1612414, train loss 2.976891, val loss 3.024092\n",
      "epoch 418, total batch 1616199, train loss 2.976100, val loss 3.031600\n",
      "epoch 419, total batch 1619984, train loss 2.970674, val loss 3.028260\n",
      "epoch 420, total batch 1623769, train loss 2.963367, val loss 3.028149\n",
      "epoch 421, total batch 1627554, train loss 2.969679, val loss 3.030318\n",
      "epoch 422, total batch 1631339, train loss 2.969139, val loss 3.026487\n",
      "epoch 423, total batch 1635124, train loss 2.976247, val loss 3.030448\n",
      "epoch 424, total batch 1638909, train loss 2.977129, val loss 3.025786\n",
      "epoch 425, total batch 1642694, train loss 2.962593, val loss 3.029732\n",
      "epoch 426, total batch 1646479, train loss 2.970298, val loss 3.028161\n",
      "epoch 427, total batch 1650264, train loss 2.954420, val loss 3.027613\n",
      "epoch 428, total batch 1654049, train loss 2.963147, val loss 3.017214\n",
      "epoch 429, total batch 1657834, train loss 2.965765, val loss 3.024349\n",
      "epoch 430, total batch 1661619, train loss 2.969187, val loss 3.026431\n",
      "epoch 431, total batch 1665404, train loss 2.964159, val loss 3.023123\n",
      "epoch 432, total batch 1669189, train loss 2.964658, val loss 3.018863\n",
      "epoch 433, total batch 1672974, train loss 2.958089, val loss 3.027049\n",
      "epoch 434, total batch 1676759, train loss 2.972087, val loss 3.019950\n",
      "epoch 435, total batch 1680544, train loss 2.955986, val loss 3.017279\n",
      "epoch 436, total batch 1684329, train loss 2.958038, val loss 3.016295\n",
      "epoch 437, total batch 1688114, train loss 2.951731, val loss 3.011091\n",
      "epoch 438, total batch 1691899, train loss 2.954927, val loss 3.013659\n",
      "epoch 439, total batch 1695684, train loss 2.956227, val loss 3.011992\n",
      "epoch 440, total batch 1699469, train loss 2.960503, val loss 3.005615\n",
      "epoch 441, total batch 1703254, train loss 2.944818, val loss 3.024114\n",
      "epoch 442, total batch 1707039, train loss 2.949315, val loss 3.010285\n",
      "epoch 443, total batch 1710824, train loss 2.953699, val loss 3.010083\n",
      "epoch 444, total batch 1714609, train loss 2.948114, val loss 3.015871\n",
      "epoch 445, total batch 1718394, train loss 2.953162, val loss 3.013305\n",
      "epoch 446, total batch 1722179, train loss 2.950631, val loss 3.005668\n",
      "epoch 447, total batch 1725964, train loss 2.950087, val loss 3.006949\n",
      "epoch 448, total batch 1729749, train loss 2.947362, val loss 3.005886\n",
      "epoch 449, total batch 1733534, train loss 2.951657, val loss 3.005956\n",
      "epoch 450, total batch 1737319, train loss 2.949699, val loss 3.007644\n",
      "epoch 451, total batch 1741104, train loss 2.944210, val loss 3.008706\n",
      "epoch 452, total batch 1744889, train loss 2.945133, val loss 3.012188\n",
      "epoch 453, total batch 1748674, train loss 2.939828, val loss 3.007779\n",
      "epoch 454, total batch 1752459, train loss 2.948769, val loss 3.001037\n",
      "epoch 455, total batch 1756244, train loss 2.952120, val loss 3.008744\n",
      "epoch 456, total batch 1760029, train loss 2.945776, val loss 3.000744\n",
      "epoch 457, total batch 1763814, train loss 2.941321, val loss 3.003271\n",
      "epoch 458, total batch 1767599, train loss 2.942063, val loss 3.000342\n",
      "epoch 459, total batch 1771384, train loss 2.948684, val loss 3.006729\n",
      "epoch 460, total batch 1775169, train loss 2.937068, val loss 2.993162\n",
      "epoch 461, total batch 1778954, train loss 2.944504, val loss 2.992833\n",
      "epoch 462, total batch 1782739, train loss 2.939529, val loss 2.997668\n",
      "epoch 463, total batch 1786524, train loss 2.938682, val loss 2.995732\n",
      "epoch 464, total batch 1790309, train loss 2.935597, val loss 2.999934\n",
      "epoch 465, total batch 1794094, train loss 2.937463, val loss 2.993340\n",
      "epoch 466, total batch 1797879, train loss 2.927077, val loss 2.987489\n",
      "epoch 467, total batch 1801664, train loss 2.933581, val loss 2.992256\n",
      "epoch 468, total batch 1805449, train loss 2.934102, val loss 2.983946\n",
      "epoch 469, total batch 1809234, train loss 2.935614, val loss 2.995393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 470, total batch 1813019, train loss 2.945168, val loss 2.994587\n",
      "epoch 471, total batch 1816804, train loss 2.933258, val loss 2.995372\n",
      "epoch 472, total batch 1820589, train loss 2.937829, val loss 3.000527\n",
      "epoch 473, total batch 1824374, train loss 2.934784, val loss 2.987837\n",
      "epoch 474, total batch 1828159, train loss 2.931052, val loss 2.993116\n",
      "epoch 475, total batch 1831944, train loss 2.932025, val loss 2.992403\n",
      "epoch 476, total batch 1835729, train loss 2.929716, val loss 2.996933\n",
      "epoch 477, total batch 1839514, train loss 2.937859, val loss 2.989053\n",
      "epoch 478, total batch 1843299, train loss 2.932791, val loss 2.991015\n",
      "epoch 479, total batch 1847084, train loss 2.924136, val loss 2.984659\n",
      "epoch 480, total batch 1850869, train loss 2.931757, val loss 2.981525\n",
      "epoch 481, total batch 1854654, train loss 2.933140, val loss 2.988127\n",
      "epoch 482, total batch 1858439, train loss 2.917541, val loss 2.986557\n",
      "epoch 483, total batch 1862224, train loss 2.913023, val loss 2.986529\n",
      "epoch 484, total batch 1866009, train loss 2.931866, val loss 2.982541\n",
      "epoch 485, total batch 1869794, train loss 2.921393, val loss 2.982688\n",
      "epoch 486, total batch 1873579, train loss 2.928437, val loss 2.978730\n",
      "epoch 487, total batch 1877364, train loss 2.923389, val loss 2.984606\n",
      "epoch 488, total batch 1881149, train loss 2.919695, val loss 2.987061\n",
      "epoch 489, total batch 1884934, train loss 2.923664, val loss 2.977599\n",
      "epoch 490, total batch 1888719, train loss 2.920180, val loss 2.986915\n",
      "epoch 491, total batch 1892504, train loss 2.932255, val loss 2.981174\n",
      "epoch 492, total batch 1896289, train loss 2.915955, val loss 2.973571\n",
      "epoch 493, total batch 1900074, train loss 2.921940, val loss 2.975887\n",
      "epoch 494, total batch 1903859, train loss 2.920540, val loss 2.977611\n",
      "epoch 495, total batch 1907644, train loss 2.913513, val loss 2.982399\n",
      "epoch 496, total batch 1911429, train loss 2.921449, val loss 2.982462\n",
      "epoch 497, total batch 1915214, train loss 2.914017, val loss 2.973390\n",
      "epoch 498, total batch 1918999, train loss 2.915328, val loss 2.980059\n",
      "epoch 499, total batch 1922784, train loss 2.910188, val loss 2.974605\n",
      "epoch 500, total batch 1926569, train loss 2.908618, val loss 2.975044\n"
     ]
    }
   ],
   "source": [
    "embed_weights, nce_weights = model.train(X, Y, X_val, Y_val,\n",
    "                                         graph_params['epoch'],\n",
    "                                         graph_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_embed = word2vec.Word2Vec(embed_weights, dictionary)\n",
    "word_vector_nce = word2vec.Word2Vec(nce_weights, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer: 8 closest words to: 'mahathir'\n",
      "['zambry', 'subramaniam', 'noor', 'hilmi', 'ghizan', 'maamor', 'azmin', 'johari'] \n",
      "\n",
      "NCE layer: 8 closest words to: 'mahathir'\n",
      "['maszlee', 'subramaniam', 'najib', 'dzulkefly', 'zambry', 'anwar', 'rosmah', 'daim']\n"
     ]
    }
   ],
   "source": [
    "word = 'mahathir'\n",
    "print(\"Embedding layer: 8 closest words to: '%s'\"%(word))\n",
    "print(word_vector_embed.n_closest(word=word, num_closest=8, metric='cosine'), '\\n')\n",
    "print(\"NCE layer: 8 closest words to: '%s'\"%(word))\n",
    "print(word_vector_nce.n_closest(word=word, num_closest=8, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kerajaan', 'bank', 'ibu', 'seluruh', 'sosiobudaya']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_nce.analogy('mahathir', 'anwar', 'kerajaan', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blogger', 'rakyat', 'johormb', 'penduduk', 'suporter']\n"
     ]
    }
   ],
   "source": [
    "print(word_vector_embed.analogy('mahathir', 'anwar', 'kerajaan', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = [rev_dictionary[i] for i in word_array[12200:12300]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " sangat dominan sedangkan mca ada masalah mic dan gerakan ada masalah ini yang lemahkan bnnamun nature politik memang begitu tapi boleh settle down lepas prucuma bimbang orang malaysia sangat emosi dengan apa yang berlaku maka ia tidak sihatazizudin politik kena rasional jadi kita patut ke arah politik matangnamun hakikatnya melihat pada isu kaum ia berlaku di semua negarakena rasionalkan politik perkauman itu cuma di malaysia kita masih baik dan tidak berperang sesama sendirilihat faktor ekonomi mempengaruhi politik jadi fenomena arab spring tidak akan berlaku di malaysiamoderator adakah bn boleh reformmaszlee kalau kita lihat bn dan umno ada dinamik berevolusi ketika\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = word2vec.build_training_set(word_array[(12200-2):(12300+2)])\n",
    "y_hat = model.sess.run(model.logits,feed_dict={model.X:x})\n",
    "passage_predict = [rev_dictionary[i] for i in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lebih benar oleh tidak berkata presiden kesihatan dan tidak mengenai perkara ini yang dilakukan dan masyarakat itu yang benar ini akan menjadi berita yang dan lagi rakyat yang dan tidak dan isu yang baik kerana ia tidak ada islam yang ada dan tidak berlaku ke peringkat itu kita dan demikian beberapa harga itu yang kerana di kalangan isu menggunakan ini politik dan mereka di negara yang lebih ada dan perlu ada dengan dan isu yang dan minyak dan isu arab itu tidak akan dilakukan di sini pihak mereka akan berlaku kerana kita ada rakyat dan tidak tidak dan dan kepada\n"
     ]
    }
   ],
   "source": [
    "readable = ''\n",
    "for word in passage_predict:\n",
    "    if word == '\"':\n",
    "        readable += word\n",
    "    elif word in ['?', '!', '.', ',']:\n",
    "        readable += word + ' '\n",
    "    else: \n",
    "        readable += ' ' + word\n",
    "print(readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('word2vec-32.p', 'wb') as fopen:\n",
    "    pickle.dump({'dictionary':dictionary,'rev_dictionary':rev_dictionary,\n",
    "                 'embed_weights':embed_weights,'nce_weights':nce_weights}, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
