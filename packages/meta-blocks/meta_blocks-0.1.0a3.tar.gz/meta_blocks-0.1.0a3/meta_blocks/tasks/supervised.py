"""Supervised tasks and task distributions."""
import logging
from typing import List, Optional, Tuple

import numpy as np
import tensorflow.compat.v1 as tf

from meta_blocks.datasets.base import ClfDataset, ClfMetaDataset
from meta_blocks.samplers.base import Sampler
from meta_blocks.tasks import base

logger = logging.getLogger(__name__)

# Transition to V2 will happen in stages.
tf.disable_v2_behavior()
tf.enable_resource_variables()

__all__ = ["SupervisedTask", "SupervisedTaskDistribution"]


class SupervisedTask(base.Task):
    """A task for supervised meta-learning.

    The Task is layer above a Dataset generated by a MetaDataset. Essentially,
    Datasets provide access to an infinite flow of data from their underlying
    Categories. Tasks manage that flow and define the following components:

    * Query set: a balanced sample (1 per class) of labeled examples.
      Technically, this set defines the classes.
    * (Unlabeled) support set: a (large) set of unlabeled examples.
      Basically, the rest of data that are not in the query set.
    * (Labeled) support set: a (small) set of labeled examples.
      A few samples from the full support set that are labeled.

    Notes
    -----
    * Even though the task is supervised, the full support is unlabeled.
      The labeled support set is a subset selected from the full support set
      using internal placeholder `self._support_labeled_ids` that must be fed in
      at execution time. Note that we refer to labeled support tensors as just
      `support_tensors` and to unlabeled as `unlabeled_support_inputs`.

    * The dataset underlying a supervised task can be fairly large (e.g., mini-
      ImageNet has 600 samples per category, so that 20-way datasets have
      12,000 samples each). Pre-processing inputs in some cases might be
      computationally expensive (e.g., decoding and resizing images, applying
      optional perturbations, etc.) which is further multiplied by the size
      of the meta-batch. Most of the times, processing of the entire dataset
      is unnecessary, since learning and adaptation is performed using only
      small (labeled) support and query sets, but sometimes it may be necessary.
      Thus, the class provides a flexible access to raw and processed tensors
      that represent support and query sets.

    Parameters
    ----------
    dataset : ClfDataset

    num_query_shots : int, optional (default: 1)

    name: str, optional
    """

    def __init__(
        self, dataset: ClfDataset, num_query_shots: int = 1, name: Optional[str] = None
    ):
        super(SupervisedTask, self).__init__(
            dataset=dataset,
            num_query_shots=num_query_shots,
            name=(name or self.__class__.__name__),
        )

        # Internals.
        self._support_labeled_ids = None
        self._support_inputs_all = None
        self._support_labels_all = None
        self._support_tensors = None
        self._query_tensors = None

    # --- Properties. ---

    @property
    def num_ways(self) -> int:
        """Returns the number of classification ways."""
        return self.dataset.num_classes

    @property
    def query_size(self) -> int:
        """Returns the size of the query set."""
        return self.num_query_shots * self.num_ways

    @property
    def support_size(self) -> tf.Tensor:
        """Returns the size of the (labeled) support set."""
        return tf.size(self._support_labeled_ids)

    @property
    def unlabeled_support_size(self) -> tf.Tensor:
        """Returns the size of the full unlabeled support set."""
        return tf.shape(self._support_inputs_all)[0]

    @property
    def support_tensors(self) -> Tuple[tf.Tensor, tf.Tensor]:
        """Returns a tuple of support (inputs, labels) tensors."""
        return self._support_tensors

    @property
    def query_tensors(self) -> Tuple[tf.Tensor, tf.Tensor]:
        """Returns a tuple of query (inputs, labels) tensors."""
        return self._query_tensors

    @property
    def unlabeled_support_inputs(self) -> tf.Tensor:
        """Returns a tensor of all unlabeled support inputs from the dataset."""
        return self._support_inputs_all

    # --- Methods. ---

    @staticmethod
    def _get_inputs_and_labels(
        data_tensors: Tuple[tf.Tensor],
        start_index: int,
        end_index: Optional[int] = None,
    ) -> tf.Tensor:
        """Slices data tensors of the dataset and constructs inputs and labels.

        Note that `self.dataset` stores a tuple of `data_tensors`, where each
        tensor stores inputs of the corresponding class. This method slices
        `data_tensors`, concatenates them to construct inputs, and builds
        the corresponding labels tensor.
        """
        inputs = [x[start_index:end_index] for x in data_tensors]
        labels = [tf.fill(tf.shape(x)[:1], k) for k, x in enumerate(inputs)]
        return tf.concat(inputs, axis=0), tf.concat(labels, axis=0)

    def _build(self):
        """Builds internals of the task."""
        # Build a placeholder for labeled support ids.
        self._support_labeled_ids = tf.placeholder(
            tf.int32, shape=[None], name="support_labeled_ids"
        )
        # Build query tensors.
        start_index = 0
        end_index = start_index + self.num_query_shots
        self._query_tensors = self._get_inputs_and_labels(
            self.dataset.data_tensors, start_index, end_index
        )
        # Build support tensors.
        start_index = end_index
        (
            self._support_inputs_all,
            self._support_labels_all,
        ) = self._get_inputs_and_labels(self.dataset.data_tensors, start_index, None)
        # Build labeled support tensors.
        self._support_tensors = tuple(
            map(
                lambda x: tf.gather(x, self._support_labeled_ids, axis=0),
                (self._support_inputs_all, self._support_labels_all),
            )
        )

    def get_feed_list(
        self, support_labeled_ids: np.ndarray
    ) -> List[Tuple[tf.Tensor, np.ndarray]]:
        """Creates a feed list needed for the task."""
        feed_list = [(self._support_labeled_ids, support_labeled_ids)]
        return feed_list


class SupervisedTaskDistribution(base.TaskDistribution):
    """The base class for supervised task distributions.

    Parameters
    ----------
    meta_dataset : ClfMetaDataset

    num_query_shots : int, optional (default: 1)

    num_support_shots : int, optional (default: 1)

    name: str, optional
    """

    def __init__(
        self,
        meta_dataset: ClfMetaDataset,
        sampler: Sampler,
        num_query_shots: int = 1,
        num_support_shots: int = 1,
        name: Optional[str] = None,
    ):
        super(SupervisedTaskDistribution, self).__init__(
            meta_dataset=meta_dataset,
            num_query_shots=num_query_shots,
            name=(name or self.__class__.__name__),
        )
        self.num_support_shots = num_support_shots
        self.sampler = sampler

        # Internals.
        self.num_requested_labels = None
        self._requests = None
        self._requested_ids = None

    # --- Properties. ---

    @property
    def query_labels_per_task(self) -> int:
        return self.num_classes * self.num_query_shots

    @property
    def support_labels_per_task(self) -> int:
        return self.num_classes * self.num_support_shots

    # --- Methods. ---

    def _build(self):
        """Builds internals."""
        self.task_batch = tuple(
            SupervisedTask(
                dataset=dataset,
                num_query_shots=self.num_query_shots,
                name=f"SupervisedTask{i}",
            ).build()
            for i, dataset in enumerate(self.meta_dataset.dataset_batch)
        )

    def initialize(self, **kwargs):
        """Initializes task distribution."""
        self._requests = []
        self._requested_ids = []
        self.num_requested_labels = 0

        # Build the sampler.
        self.sampler.build(tasks=self.task_batch, **kwargs)
